{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "KoGPT2 Chat ONNX",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "k02aDKk6gljR"
      },
      "source": [
        "# 패키지 설치\r\n",
        "!pip install mxnet==1.6.0 gluonnlp==0.9.1 sentencepiece==0.1.91 pandas==1.0.5 transformers==2.11.0 pytorch_lightning==0.8.1 onnx onnxruntime"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zDvLK-WsgnUz"
      },
      "source": [
        "# KoGPT2 Chat Convert-To-ONNX Train\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lUK3VNsogcfK"
      },
      "source": [
        "import torch\r\n",
        "import numpy as np\r\n",
        "from kogpt2.pytorch_kogpt2 import get_pytorch_kogpt2_model\r\n",
        "from gluonnlp.data import SentencepieceTokenizer\r\n",
        "from kogpt2.utils import get_tokenizer\r\n",
        "from pytorch_lightning.core.lightning import LightningModule\r\n",
        "import onnxruntime\r\n",
        "\r\n",
        "tok_path = get_tokenizer()\r\n",
        "model, vocab = get_pytorch_kogpt2_model()\r\n",
        "tok = SentencepieceTokenizer(tok_path, num_best=0, alpha=0)\r\n",
        "\r\n",
        "class KoGPT2Chat(LightningModule):\r\n",
        "    def __init__(self, hparams, **kwargs):\r\n",
        "        super(KoGPT2Chat, self).__init__()\r\n",
        "        self.hparams = hparams\r\n",
        "        self.tok_path = tok_path\r\n",
        "        self.neg = -1e18\r\n",
        "        self.kogpt2, self.vocab = model, vocab\r\n",
        "\r\n",
        "        self.loss_function = torch.nn.CrossEntropyLoss(reduction='none')\r\n",
        "\r\n",
        "    def forward(self, inputs):\r\n",
        "        # (batch, seq_len, hiddens)\r\n",
        "        output, _ = self.kogpt2(inputs)\r\n",
        "        return output\r\n",
        "\r\n",
        "if __name__ == '__main__':\r\n",
        "    # init\r\n",
        "    U_TKN = '<usr>'\r\n",
        "    S_TKN = '<sys>'\r\n",
        "    BOS = '<s>'\r\n",
        "    EOS = '</s>'\r\n",
        "    MASK = '<unused0>'\r\n",
        "    SENT = '<unused1>'\r\n",
        "\r\n",
        "    # PATH\r\n",
        "    MODEL_ONNX_PATH = \"./model_chp/sample.onnx\"\r\n",
        "    # ONNX Settings\r\n",
        "    OPERATOR_EXPORT_TYPE = torch._C._onnx.OperatorExportTypes.ONNX\r\n",
        "    OPSET_VERSION = 10\r\n",
        "\r\n",
        "    gpt_chat_model = KoGPT2Chat.load_from_checkpoint(\"./model_chp/model_last.ckpt\")\r\n",
        "\r\n",
        "    sent_tokens = tok(\"\")\r\n",
        "    sent='0'\r\n",
        "    toked = tok(sent)\r\n",
        "    q_tok = tok(\"\")\r\n",
        "    a = ''\r\n",
        "    a_tok = []\r\n",
        "\r\n",
        "    input_ids = torch.LongTensor([\r\n",
        "                        vocab[U_TKN]] + vocab[q_tok] +\r\n",
        "                        vocab[EOS, SENT] + vocab[sent_tokens] +\r\n",
        "                        vocab[EOS, S_TKN] +\r\n",
        "                        vocab[a_tok]).unsqueeze(dim=0)\r\n",
        "\r\n",
        "    # export\r\n",
        "    torch.onnx.export(gpt_chat_model,\r\n",
        "                      input_ids,\r\n",
        "                      MODEL_ONNX_PATH,\r\n",
        "                      verbose=True,\r\n",
        "                      operator_export_type=OPERATOR_EXPORT_TYPE,\r\n",
        "                      input_names=['input_ids'],\r\n",
        "                      output_names=['output'],\r\n",
        "                      dynamic_axes={\r\n",
        "                          'input_ids': {\r\n",
        "                              1: \"sequence\"\r\n",
        "                          },\r\n",
        "                          'output': {\r\n",
        "                              1: \"sequence\"\r\n",
        "                          }\r\n",
        "                      },\r\n",
        "                      opset_version=OPSET_VERSION)\r\n",
        "    print(\"Export of kogpt2 onnx complete!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7AhTGJZgv5i"
      },
      "source": [
        "# KoGPT2 Chat Convert-To-ONNX Generate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hXCApmDTgWsQ"
      },
      "source": [
        "import torch\r\n",
        "from kogpt2.pytorch_kogpt2 import get_pytorch_kogpt2_model\r\n",
        "from kogpt2.utils import get_tokenizer\r\n",
        "import onnxruntime\r\n",
        "from gluonnlp.data import SentencepieceTokenizer\r\n",
        "import numpy as np\r\n",
        "import time\r\n",
        "\r\n",
        "print(onnxruntime.get_device())\r\n",
        "\r\n",
        "# 폴더 이동\r\n",
        "%cd /content/drive/MyDrive/Colab Notebooks/gpt/KoGPT2-chatbot-master\r\n",
        "\r\n",
        "MODEL_ONNX_PATH = \"./model_chp/sample.onnx\"\r\n",
        "_, vocab = get_pytorch_kogpt2_model()\r\n",
        "model = onnxruntime.InferenceSession(MODEL_ONNX_PATH)\r\n",
        "tok_path = get_tokenizer()\r\n",
        "tok = SentencepieceTokenizer(tok_path, num_best=0, alpha=0)\r\n",
        "\r\n",
        "U_TKN = '<usr>'\r\n",
        "S_TKN = '<sys>'\r\n",
        "BOS = '<s>'\r\n",
        "EOS = '</s>'\r\n",
        "MASK = '<unused0>'\r\n",
        "SENT = '<unused1>'\r\n",
        "sent_tokens = tok(\"\")\r\n",
        "\r\n",
        "while 1:\r\n",
        "  q = input('user > ').strip()\r\n",
        "  if q == 'quit':\r\n",
        "    break\r\n",
        "  # if len(q) > 1000:\r\n",
        "  #   print(\"Too many sentences !\")\r\n",
        "  #   break\r\n",
        "  t = time.time()\r\n",
        "  q_tok = tok(q)\r\n",
        "  a = ''\r\n",
        "  a_tok = []\r\n",
        "  while 1:\r\n",
        "    input_ids = torch.LongTensor([\r\n",
        "        vocab[U_TKN]] + vocab[q_tok] +\r\n",
        "        vocab[EOS, SENT] + vocab[sent_tokens] +\r\n",
        "        vocab[EOS, S_TKN] +\r\n",
        "        vocab[a_tok]).unsqueeze(dim=0)\r\n",
        "    pred = model.run(None, {'input_ids': np.array(input_ids)})\r\n",
        "    gen = vocab.to_tokens(\r\n",
        "        torch.argmax(\r\n",
        "            torch.tensor(pred),\r\n",
        "            dim=-1).squeeze().numpy().tolist())[-1]\r\n",
        "    if gen == EOS:\r\n",
        "        break\r\n",
        "    a += gen.replace('▁', ' ')\r\n",
        "    a_tok = tok(a)\r\n",
        "  print(\"Length of sentences : \", len(q))\r\n",
        "  print(\"Running time : \", time.time() - t)\r\n",
        "  print(\"Simsimi > {}\".format(a.strip()))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}